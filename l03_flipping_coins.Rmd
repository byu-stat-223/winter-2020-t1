---
title: "More Flipping Coins"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

Consider flipping `size` distinct coins. After each round, each coin that is tails
is flipped again. After `n_rounds`, where `n_rounds` is at least one, the proportion
of coins that are `heads` is computed. Answer these questions:

1. What is the expected number of heads?
2. What is the standard deviation of the number of heads?

In order to answer these questions, we'll build a function to help us simulate 
this scenario.

```{r}
flip_coins <- function(size = 64, n_rounds = 1) {
  if (n_rounds < 1) stop("n_rounds must be at least 1.")
  x <- sample(c("H", "T"), size = size, replace = TRUE)
  if (n_rounds > 1) {
    for (i in 2:n_rounds) {
      x[x == "T"] <- sample(c("H", "T"), size = sum(x == "T"), replace = TRUE)
    }
  }
  sum(x == "H")
}

flip_coins(n_rounds = 3)
```

How can we estimate the average number of heads and the standard deviation
associated with number of heads?  We can certainly use probability theory. An 
alternative approach is a Monte Carlo or simulation study.

```{r}
n_reps <- 1000
n_rounds <- 3
```

#### For Loop
```{r}
library(tictoc)

n_heads <- numeric(n_reps)
tic("For Loop")
for (i in seq_along(n_heads)) {
  n_heads[i] <- flip_coins(n_rounds = n_rounds)
}
toc()
```


#### sapply / replicate
```{r}
tic("sapply")
n_heads <- sapply(1:n_reps, function(x) flip_coins(n_rounds = n_rounds))
toc()

tic("replicate")
n_heads <- replicate(n_reps, flip_coins(n_rounds = n_rounds))
toc()
```

#### Tidyverse
```{r}
library(tidyverse)
tic("purrr")
n_heads <- rerun(n_reps, flip_coins(n_rounds = n_rounds)) %>% 
  unlist()
toc()
```

Now that we have the sampling distribution of the number of heads (`n_heads`), 
we can summarise it.

```{r}
plot(table(n_heads))
```

```{r}
table(n_heads) / length(n_heads)
```

```{r}
mean(n_heads)
sd(n_heads)
```

Is there uncertainty in these summaries? Yes! So, how do we understand and 
communicate those uncertainties? Well, we could just wrap our simulation inside
another simulation!

![](images/simulation-meme.jpg)

Now we're going to produce 1000 Monte Carlo estimates of the mean and standard
deviation.

```{r}
mc_estimates <- replicate(1000, {
  n_heads <- replicate(n_reps, flip_coins(n_rounds = 3))
  c(mean(n_heads), sd(n_heads))
})
dim(mc_estimates)
```

```{r}
mc_means <- mc_estimates[1,]
mc_sds <- mc_estimates[2,]
```


Now we can look at the distribution of our simulation estimates.

```{r}
hist(mc_means, main = "Mean")
```

```{r}
hist(mc_sds, main = "SD")
```

Now we can compute a 95% confidence interval for these estimates.

```{r}
quantile(mc_means, c(0.025, 0.975))
quantile(mc_sds, c(0.025, 0.975))
```

That worked!! But, it was exceptionally computationally intensive (1000 * 1000
simulations). Is there a better way to assess this uncertainty without going through
all of this? Yes! We can invoke the Central Limit Theorem. Since `n_reps` is large,
the CLT applies. When our sample size grows large, the distribution of our sample
means is approximately normally distributed with $\mu_{\bar{x}} = \mu$ and $\sigma_{\bar{x}} = \sigma / \sqrt{n}$.

```{r}
simulation_ci <- function(x, confidence_level = 0.95) {
  n <- length(x)
  m <- mean(x)
  s <- sd(x)
  area <- 1 - (1 - confidence_level)/2
  m + c(-1, 1) * qnorm(area) * s / sqrt(n)
}
```

The function above simply calculates a confidence interval by implementing:
$$
C.I. = \bar{x}\pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}
$$

Where $\bar{x}$ is the sample mean and $s$ is the sample standard deviation, and
$z_{\alpha/2}$ is the Z score for the desired confidence level $\alpha$. This Z
score can be calculated using `qnorm()`.

```{r}
simulation_ci(n_heads)
```

Note that we can also extend this same formula to provide a confidence interval 
estimate for the standard deviation of `n_heads`. Given that the standard deviation
is defined as $\sqrt{\frac{1}{N}\sum_{i=1}^N(x_i - \mu)^2}$ then we can do the following:
```{r}
sqrt(simulation_ci((n_heads - mean(n_heads))^2))
```

Now we have a way of assessing the simulation error without running an incredible
amount of simulations, and it's all thanks to the Central Limit Theorem.
